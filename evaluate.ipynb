{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rMcGf3r6zby"
   },
   "source": [
    "# Question Answering over Resources\n",
    "The notebook is based on https://python.langchain.com/en/latest/modules/chains/index_examples/qa_with_sources.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAYYChTe6zb1"
   },
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FimqAIgc6zb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the token for Google Bard········\n",
      "Enter the token for OpenAI········\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "import getpass\n",
    "from bardapi import Bard\n",
    "import os\n",
    "\n",
    "bard_token = getpass.getpass(\"Enter the token for Google Bard\")\n",
    "openai_token = getpass.getpass(\"Enter the token for OpenAI\")\n",
    "# Input OpenAI API Key Here\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvJlaM8n7k_R",
    "outputId": "20f32fe4-2c9f-47d1-8567-0852f2e11e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('test_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "data = data['data']\n",
    "num = len(data)\n",
    "print(num)\n",
    "# print(data[0]['paragraphs'][0]['qas'][0])\n",
    "print(data[0]['paragraphs'][0]['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========\n",
    "# QUESTION: In what country is Normandy located?\n",
    "# ANSWER1: France\n",
    "# ANSWER2: Normandy is located in France.\n",
    "# RESPONSE: Yes\n",
    "# ==========\n",
    "# QUESTION: From which countries did the Norse originate?\n",
    "# ANSWER1: Denmark, Iceland and Norway \n",
    "# ANSWER2: The Norse who originated the Normans came from Denmark, Iceland, and Norway.\n",
    "# RESPONSE: Yes\n",
    "# ==========\n",
    "# QUESTION: What is France a region of?\n",
    "# ANSWER1: No relevant information found \n",
    "# ANSWER2: Sorry, I don't know.\n",
    "# RESPONSE: Yes\n",
    "# =========\n",
    "# QUESTION: What is France a region of?\n",
    "# ANSWER1: No relevant information found \n",
    "# ANSWER2: France is a region of Normandy.\n",
    "# RESPONSE: No\n",
    "# =========\n",
    "\n",
    "#Along with the answer, please also give a confidence score from 1 to 10 about how confident you are about your answer.\n",
    "\n",
    "eval_text_template = '''\n",
    "You are not allowed to use any outside knowledge. You can only use what is given to you. Given one question and two answers, please response whether these ANSWER1 and ANSWER2 have the same meaning under the given question. \n",
    "The only output you can give is either \"Yes\" or \"No\". Please output only one single word. No explanation needed.\n",
    "If ANSWER1 says \"No relevant information found\", you can only say \"yes\" when ANSWER2 also indicates the question cannot be answered. \n",
    "\n",
    "QUESTION: {question}\n",
    "ANSWER1: {answer}\n",
    "ANSWER2: {res_text}\n",
    "'''\n",
    "\n",
    "# ask_template = '''\n",
    "# Please answer the following question only based on the given context. Do NOT use any outside knowledge. \n",
    "# If you are unsure about the answer or you cannot find the information to answer your question, you must say \"Sorry, I cannot answer this question\"\n",
    "# ==========\n",
    "# QUESTION: In what country is Normandy located?\n",
    "# RESPONSE: France\n",
    "# ==========\n",
    "\n",
    "# QUESTION: What is France a region of?\n",
    "# RESPONSE: Sorry, I cannot answer this question\n",
    "# =========\n",
    "# QUESTION: {question}\n",
    "# RESPONSE: \n",
    "# '''\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based on the context below. You are NOT allowed to use any outside information. If the question cannot be answered using the information provided, you must answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"query\"],\n",
    "    template=template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_NhDW6QV6zb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengzhong/opt/anaconda3/envs/qa/lib/python3.11/site-packages/langchain/llms/openai.py:171: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/zhengzhong/opt/anaconda3/envs/qa/lib/python3.11/site-packages/langchain/llms/openai.py:716: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: In what country is Normandy located?\n",
      "answer:France \n",
      "response:Sure, I can do that. The paragraph you provided states that Normandy is a region in France. Therefore, the answer to the question \"In what country is Normandy located?\" is **France**.\r\n",
      "\r\n",
      "The paragraph also mentions that the Normans were descended from Norse raiders and pirates from Denmark, Iceland, and Norway. However, the paragraph does not state that Normandy is located in any of these countries. In fact, the paragraph specifically states that Normandy is a region in France.\r\n",
      "\r\n",
      "Therefore, the answer to the question is France.\n",
      " \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 48\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;66;03m# long text----- search similarity first\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#             embeddings = OpenAIEmbeddings()\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#             text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=0)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#             in_text = prompt_template.format(context=text, query=question)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#             res_text = model(in_text)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m             prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI want you to work as a question answering machine over given documents. The documents you will be given is the following paragraph: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Then, I want to you answer the question based on the given paragraph. The question is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 48\u001b[0m             res_text2 \u001b[38;5;241m=\u001b[39m \u001b[43mbard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#             print(res_text+\"||||\"+res_text2)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#             time.sleep(1)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#             is_same = evaluator.get_answer(eval_text_template.format(question=question, answer=res_text2, res_text=res_text))['content']\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#                 else:\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#                     hallucination.append(0)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124manswer:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mresponse:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres_text2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa/lib/python3.11/site-packages/bardapi/core.py:128\u001b[0m, in \u001b[0;36mBard.get_answer\u001b[0;34m(self, input_text)\u001b[0m\n\u001b[1;32m    122\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf.req\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps([\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m.\u001b[39mdumps(input_text_struct)]),\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSNlM0e,\n\u001b[1;32m    125\u001b[0m }\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Get response\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://bard.google.com/_/BardChatUi/data/assistant.lamda.BardFrontendService/StreamGenerate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Post-processing of response\u001b[39;00m\n\u001b[1;32m    137\u001b[0m resp_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(resp\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39msplitlines()[\u001b[38;5;241m3\u001b[39m])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa/lib/python3.11/site-packages/requests/sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa/lib/python3.11/site-packages/urllib3/connection.py:454\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa/lib/python3.11/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from bardapi import Bard\n",
    "import time\n",
    "hallucination = []\n",
    "acc = []\n",
    "\n",
    "# model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')\n",
    "model = OpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\",)\n",
    "chain = load_qa_with_sources_chain(model, chain_type=\"stuff\")\n",
    "# chain = load_qa_with_sources_chain(model, chain_type=\"map_reduce\")\n",
    "evaluator = Bard(token=bard_token)\n",
    "bard = Bard(token=bard_token)\n",
    "for i in range(num):\n",
    "    for j in range(len(data[0]['paragraphs'])):\n",
    "        text= data[i]['paragraphs'][j]['context']\n",
    "#         text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "#         texts = text_splitter.split_text(text)\n",
    "#         embeddings = OpenAIEmbeddings()\n",
    "#         docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))])\n",
    "        for k in range(len(data[i]['paragraphs'][j]['qas'])):\n",
    "            question = data[i]['paragraphs'][j]['qas'][k]['question']\n",
    "            if not data[i]['paragraphs'][j]['qas'][k]['is_impossible']:\n",
    "                answer = data[i]['paragraphs'][j]['qas'][k]['answers'][0]['text']\n",
    "                pos = data[i]['paragraphs'][j]['qas'][k]['answers'][0]['answer_start']\n",
    "            else:\n",
    "                answer = \"I don't know\"\n",
    "                pos = None\n",
    "            query = question\n",
    "            \n",
    "            # long text----- search similarity first\n",
    "#             embeddings = OpenAIEmbeddings()\n",
    "#             text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=0)\n",
    "#             pages = text_splitter.split_text(text)\n",
    "#             db = Chroma.from_texts(pages, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(pages))])\n",
    "#             docs = db.similarity_search(query)\n",
    "#             model = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "#             chain = load_qa_with_sources_chain(model, chain_type=\"map_reduce\")\n",
    "#             res_text = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "            \n",
    "#             short text---- put everything in\n",
    "#             in_text = prompt_template.format(context=text, query=question)\n",
    "#             res_text = model(in_text)\n",
    "            \n",
    "            \n",
    "            \n",
    "            prompt = f\"I want you to work as a question answering machine over given documents. The documents you will be given is the following paragraph: {text}. Then, I want to you answer the question based on the given paragraph. The question is {query}\"\n",
    "            res_text2 = bard.get_answer(prompt)['content']\n",
    "#             print(res_text+\"||||\"+res_text2)\n",
    "#             time.sleep(1)\n",
    "#             is_same = evaluator.get_answer(eval_text_template.format(question=question, answer=res_text2, res_text=res_text))['content']\n",
    "#             if 'yes' not in is_same.lower():\n",
    "#                 res_text = \"I don't know\"\n",
    "#             time.sleep(0.5)\n",
    "#             result = evaluator.get_answer(eval_text_template.format(question=question, answer=answer, res_text=res_text))['content']\n",
    "#             if not data[i]['paragraphs'][j]['qas'][k]['is_impossible']:\n",
    "#                 if 'yes' in result.lower():\n",
    "#                     acc.append(1)\n",
    "#                 else:\n",
    "#                     acc.append(0)\n",
    "#             else:\n",
    "#                 if 'yes' in result.lower():\n",
    "#                     hallucination.append(1)\n",
    "#                 else:\n",
    "#                     hallucination.append(0)\n",
    "            print(f\"question: {question}\\nanswer:{answer} \\nresponse:{res_text2}\\n \\n\\n\\n\")\n",
    "#         print(f\"acc: {np.mean(acc)}, hallucination: {np.mean(hallucination)}\\n\\n\\n\")\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTJ4WEkX6zb4"
   },
   "source": [
    "## Bard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bardapi import Bard\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When were the Normans in Normandy?\"\n",
    "response = \"No relevant information found\"\n",
    "solution = \"The Normans gave their name to Normandy in the 10th and 11th centuries.\"\n",
    "\n",
    "\n",
    "\n",
    "Bard(token = token).get_answer(f\"Given the question:'{question}', do the following 2 statements have the same meaning? 1.{solution}, 2.{response}. Say yes if true. Otherwise, say no.\")['content']\n",
    "\n",
    "\n",
    "\n",
    "question = \"When was the French version of the word Norman first recorded?\"\n",
    "ans1 = \"No relevant information found \"\n",
    "ans2 = \"The French version of the word Norman was first recorded in Medieval Latin in the 9th century.\"\n",
    "\n",
    "res = bard.get_answer(f\"Answer the following question in a **single word**. Remember, **a single word**. Based on {question}, does {ans1} mean the same thing as {ans2}?\")['content']\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json, collections, time, re, string, os\n",
    "from datetime import datetime\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_url(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup\n",
    "def extract_nature_articles():\n",
    "    \"\"\"\n",
    "    Search for and parse all coronavirus-related News article from the Nature journal that were\n",
    "    published in a given period\n",
    "    \n",
    "    args:\n",
    "        start_date (str): the lower bound of the date range to filter articles,\n",
    "            has the format yyyy-mm-dd\n",
    "        end_date (str): the upper bound (inclusive) of the date range to filter articles,\n",
    "            has the format yyyy-mm-dd\n",
    "    \n",
    "    kwargs:\n",
    "        base_url (str): the home page url of Nature\n",
    "    \n",
    "    return:\n",
    "        List[str] : a list of article titles that meet the search criteria, ordered by\n",
    "            date and by title\n",
    "    \"\"\"\n",
    "    result = collections.OrderedDict()\n",
    "    url = \"https://www.nature.com/search?q=coronavirus&journal=nature&article_type=news&date_range=2023-2023&order=date_asc&title=coronavirus\"\n",
    "    soup = retrieve_url(url)\n",
    "    article_list = soup.find('ul', {'class':'app-article-list-row'}).find_all('li', {'class': \"app-article-list-row__item\"})\n",
    "    for item in article_list:\n",
    "        time = item.find('div', {'class': \"c-card__section\"}).find('time', {'class': 'c-meta__item'})['datetime']\n",
    "    \n",
    "        title = item.find('h3', {'class': \"c-card__title\"}).text.strip()\n",
    "        if time in result:\n",
    "            result[time].append(title)\n",
    "        else:\n",
    "            result[time] = [title]\n",
    "    temp = []\n",
    "    for titles in result.values():\n",
    "        temp.extend(sorted(titles))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coronavirus variant XBB.1.5 rises in the United States — is it a global threat?',\n",
       " 'The next worrisome coronavirus variant could come from China — will it get detected?',\n",
       " 'The next generation of coronavirus vaccines: a graphical guide',\n",
       " 'NIH reinstates grant for controversial coronavirus research']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_nature_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "New SARS-CoV-2 variants arise and spread with great stealth, but that hasn’t stopped Africa’s genomic sleuths from spotting a host of these threats — and alerting the rest of the world. Now an analysis details how the rapid growth in Africa’s sequencing capacity has aided global SARS-CoV-2 surveillance1. It also reveals that most variants were imported into Africa more often than they were exported from the continent.\n",
    "\n",
    "The study, published in Science, shows that “African scientists can work together to produce high-level science”, says co-author Tulio de Oliveira, a bioinformatician at Stellenbosch University in South Africa. “Before, it was almost the norm that African scientists would work with a northern partner to produce that kind of level of science.”\n",
    "\n",
    "Homegrown talent\n",
    "In March 2020, during the early days of the pandemic, fewer than 15 of the 55 countries recognized by the African Union had the necessary infrastructure to sequence genomes. Now 39 have their own sequencing facilities. And what began as a small group of researchers in Africa meeting virtually has ballooned into a consortium involving more than 400 scientists and public-health officials from across the continent and beyond working together to track the spread of SARS-CoV-2 variants and monitor changes in the variants’ genomes.\n",
    "\n",
    "As the consortium grew, so too did the number of sequenced genomes from Africa. By mid-2021, group members had sequenced more than 10,000 SARS-CoV-2 genomes collected on the continent2. By March this year, the count had reached 100,000.\n",
    "\n",
    "The authors note that this marks a major milestone in African genomic surveillance. By comparison, only around 3,700 whole HIV genomes from Africa have been sequenced and publicly shared, although the virus has been circulating for decades.\n",
    "\n",
    "“This paper is incredible,” says Jeremy Kamil, a virologist at Louisiana State University Health Shreveport. “The world needs to see more collaborations like this.”\n",
    "\n",
    "Genomes from a total of 52 African countries have been deposited in GISAID, an online repository that is the world’s largest digital database of SARS-CoV-2 sequences. Some genomes were sequenced in the countries where they were sampled, some in other African countries and others in laboratories outside Africa.\n",
    "\n",
    "The researchers found that in-country sequencing offered a clear speed advantage. The median turnaround — the time between the collection of a specimen from an infected individual and the addition of a completed sequence to GISAID — for genomes sequenced locally was 51 days. Median turnaround was 90 days for samples sent to other African countries and 113 days for those sent to facilities elsewhere in the world (see ‘Home-ground advantage’).\n",
    "\n",
    "These findings “jumped out at me right away”, says Kamil. Speed is crucial when it comes to fighting the virus, he says. If a variant such as Omicron is allowed to spread for even two or three extra weeks before it’s noticed, the world’s already-slow systems for updating vaccines will have even less time to catch up, Kamil adds.\n",
    "\n",
    "The collection of 100,000 SARS-CoV-2 genomes enabled de Oliveira and his colleagues to map when and where variants were introduced into Africa and how they had spread. The Omicron variant, for example, was first detected in South Africa in November 2021. The researchers found that although the Omicron subvariant BA.1 was exported from Africa at least 55 times — primarily to Europe and North America — it was imported into Africa at least 69 times from Europe and 102 times from North America (see ‘Virus on the go’). These import events brought the variant to African countries outside of the southern part of the continent, says study co-author Eduan Wilkinson, a bioinformatician at Stellenbosch University.\n",
    "\n",
    "Overall, the team’s analyses indicated that most SARS-CoV-2 variants were more often introduced into Africa from other parts of the world than the other way around. “The ironic part was that Africa was punished a few times from discovery of variants,” de Oliveira says. “But a great majority of the variants, including most of the introductions of Omicron, did not come from Africa.”\n",
    "\n",
    "De Oliveira and his colleagues plan to adapt the existing sequencing infrastructure to monitor other infectious viruses and bacteria that are of concern in Africa, such as the tuberculosis bacterium, HIV and the Lassa and Ebola viruses. “We have a lot of pathogens to deal with,” de Oliveira says.\n",
    "\"\"\"\n",
    "\n",
    "text2 = \"\"\"\n",
    "The US National Institutes of Health (NIH) has reinstated a grant to a highly scrutinized research organization that studies bat coronaviruses — but the agency has placed several stipulations on the scope of the research and on the organization’s accounting practices.\n",
    "\n",
    "The move caps a years-long saga that has thrust the EcoHealth Alliance, a small non-profit organization in New York City, into the political fray for its collaborations with the Wuhan Institute of Virology (WIV) in China. In April 2020, after then-US-president Donald Trump hinted that SARS-CoV-2 originated in a WIV laboratory, the NIH terminated EcoHealth’s grant. Its goal was to study how coronaviruses, such as SARS-CoV-2, jump from bats to humans. A few months later, the NIH reinstated and immediately suspended the award until certain conditions were met that, at the time, EcoHealth said were impossible to complete.\n",
    "\n",
    "Researchers who spoke to Nature applaud the renewal, adding that this type of research is essential to avert the next pandemic. They claim that the NIH’s termination and subsequent suspension were politically motivated, and that, although long overdue, this renewal ends — for now — a drama-filled exchange between the agency and EcoHealth.\n",
    "\n",
    "“It’s about goddam time,” says Gerald Keusch, associate director of the National Emerging Infectious Diseases Laboratory at Boston University in Massachusetts, who organized researchers to push back against EcoHealth’s grant termination in 2020. “The integrity of science requires a barrier against political interference,” he says.\n",
    "\n",
    "The NIH “routinely considers processes and measures for strengthening [its] oversight over federal funds” and has been working with EcoHealth to strengthen its “administrative processes to meet NIH’s expectations”, says Amanda Fine, a spokesperson for the NIH in Washington DC.\n",
    "\n",
    "Lengthy list of conditions\n",
    "Although the organization will now be able to continue its bat coronavirus research for the first time since the saga began, the NIH placed an extensive list of restrictions on the four-year, US$2.9-million award. None of the researchers who spoke to Nature had ever seen a grant with so many stipulations.\n",
    "\n",
    "Among other things, EcoHealth is specifically forbidden from performing any in-country research in China, including with the WIV, or collecting any new samples from vertebrates — such as bats. The revised grant also mandates greater scrutiny of EcoHealth’s finances and accounting practices, driven in part by a federal watchdog report, released in January, finding that EcoHealth had misreported about $90,000 in expenses. The report also faulted the NIH for improperly terminating EcoHealth’s grant.\n",
    "\n",
    "Furthermore, EcoHealth will be forbidden from performing any work that is deemed by the NIH’s parent organization, the Department of Health and Human Services (HHS), to have the potential to enhance the virulence or transmission of a virus. This restriction stems, in part, from criticism that research done at the WIV and funded by an EcoHealth subaward qualified as ‘gain of function’ research.\n",
    "\n",
    "Disputed research\n",
    "Congressional Republicans have alleged that this research, which involved attaching spike proteins from wild bat coronaviruses to an unrelated virus to determine whether the wild pathogens could infect human airway cells, should have undergone HHS review. Anthony Fauci, then-director of the National Institute for Allergy and Infectious Diseases, has said the agency concluded that these experiments did not meet the bar to undergo such review, and noted that the WIV did not intend to enhance the viruses.\n",
    "\n",
    "Virologists say this type of research is essential for developing vaccines and therapeutics against emerging pathogens and for understanding how likely a pathogen is to spark a pandemic. The NIH and HHS have been finalizing guidance that will probably tighten the oversight of such research in the United States.\n",
    "\n",
    "“I don’t know if any other single grantee from NIH has been subjected to this level of oversight,” says Peter Daszak, president of the EcoHealth Alliance. Yet he is “positive and optimistic” about the grant restart, despite all the restrictions. A key priority for the newly released funds will be analysing nearly 300 partial or complete genomes of SARS-related coronaviruses from samples that the organization collected before the funding halt, he says.\n",
    "\n",
    "Third rail of virology\n",
    "These restrictions seem reasonable, in light of the enormous public attention to and scrutiny of gain-of-function research, says Lawrence Gostin, a health-law and policy specialist at Georgetown University in Washington DC. Still, Gostin says he is surprised the agency restarted its funding for EcoHealth, given that it has been the “third rail of politics” the past few years.\n",
    "\n",
    "Angela Rasmussen, a virologist at the University of Saskatchewan in Saskatoon, Canada, says she is pleasantly surprised to see the grant renewed, but worries about the “terrible precedent” that the NIH set by “arbitrarily” terminating an award on the basis of “unfounded rumours” regarding the origins of SARS-CoV-2. She hopes that these same restrictions will not apply to other scientists doing similar work, but is encouraged by the number of research groups that are now studying coronaviruses following the COVID-19 pandemic.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from bardapi import Bard\n",
    "import time\n",
    "model = OpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\",)\n",
    "chain = load_qa_with_sources_chain(model, chain_type=\"stuff\")\n",
    "\n",
    "question = \"How many countries recognized by the African Union had the necessary infrastructure to sequence genomes.\"\n",
    "question = 'Summarize every single given article separately.'\n",
    "inlanguage = \"Chinese\"\n",
    "translated = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"You are a {inlanguage} translator.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in {inlanguage}. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level f{inlanguage} words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. The paragrah you will translate is {question}.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question = translated[\"choices\"][0][\"message\"][\"content\"]\n",
    "in_text = prompt_template.format(context=text1+\"\\n=====\\n\"+text2, query=question)\n",
    "res_text = model(in_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "背景1：本文讨论了非洲基因组测序能力的增长，使非洲科学家能够辨识和追踪SARS-CoV-2的变种。研究显示，大部分变种是从非洲输入而非输出。非洲联盟已测序了超过10万个SARS-CoV-2基因组，为全球监测提供了宝贵的数据。研究人员计划扩大测序基础设施，监测其他传染病毒和细菌。\n",
      "\n",
      "背景2：本文关注EcoHealth联盟资助的恢复，该研究机构致力于研究蝙蝠冠状病毒。由于该机构与武汉病毒研究所的合作引发了政治争议，该资助曾被终止和暂停。美国国立卫生研究院对该资助设定了一些限制条件，包括限制在中国国内的研究活动，并对该组织的财务情况进行更加严格的审查。这项研究被认为对于了解和预防未来的全球大流行病至关重要。\n"
     ]
    }
   ],
   "source": [
    "outlanguage = \"Chinese\"\n",
    "translated = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"You are a {outlanguage} translator.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in {outlanguage}. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level f{outlanguage} words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. The paragrah you will translate is {res_text}.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(translated[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Context 1: The article discusses how Africa's genomic sequencing capacity has grown, allowing African scientists to identify and track SARS-CoV-2 variants. The study shows that most variants were imported into Africa rather than exported from the continent. The African consortium has sequenced over 100,000 SARS-CoV-2 genomes, providing valuable data for global surveillance. The researchers plan to expand their sequencing infrastructure to monitor other infectious viruses and bacteria.\\n\\nContext 2: The article focuses on the reinstatement of a grant to the EcoHealth Alliance, a research organization studying bat coronaviruses. The grant had been terminated and suspended due to political controversy surrounding the organization's collaborations with the Wuhan Institute of Virology. The NIH has placed several stipulations on the grant, including restrictions on in-country research in China and greater scrutiny of the organization's finances. The research is considered essential for understanding and preventing future pandemics.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"test.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'When is the flight '\n",
    "answer = \"\"\n",
    "for page in pages:\n",
    "    text = page.page_content\n",
    "    in_text = prompt_template.format(context=text, query=question)\n",
    "    res_text = model(in_text)\n",
    "    if res_text != \"I don't know.\":\n",
    "        answer += res_text + ' '\n",
    "if len(answer) == 0:\n",
    "    answer = \"I don't know.\"\n",
    "else:\n",
    "    answer = model(f'Summarize the following text: {answer}')\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper introduces a method called EnV AE, which is an ensemble of lightweight Variational Autoencoders (VAEs) trained on subsets of the feature-space. This approach improves representation learning in high-dimensional, low sample size (HDLSS) tasks and leads to higher accuracy in downstream classification tasks. EnV AE also demonstrates better disentanglement and robustness to missing features. The method outperforms other benchmark models and introduces minimal computational overhead. The paper provides details on the implementation and performance of EnV AE on various datasets.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "chain = load_summarize_chain(ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0), chain_type=\"map_reduce\")\n",
    "chain.run(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_text': 'The content of the article includes information about PyTorch, a deep learning library, and its use in training models on various biomedical datasets. The article also provides details about the datasets used, computing resources, training details, and performance of the models on specific datasets. The article emphasizes the use of PyTorch and the training settings for each model. Additionally, the article mentions the normalization of the dataset using scikit-learn.preprocessing.MinMaxScaler and the use of specific hardware for training the models. The article concludes with a summary of the performance of the models on different datasets.\\nSOURCES: test.pdf'}\n"
     ]
    }
   ],
   "source": [
    "question = \"总结这篇文章的内容\"\n",
    "\n",
    "inlanguage = \"Chinese\"\n",
    "translated = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"You are a {inlanguage} translator.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in {inlanguage}. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level f{language} words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. The paragrah you will translate is {question}.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question = translated[\"choices\"][0][\"message\"][\"content\"]\n",
    "in_text = prompt_template.format(context=text, query=question)\n",
    "res_text = model(in_text)\n",
    "\n",
    "outlanguage = \"Chinese\"\n",
    "translated = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"You are a {outlanguage} translator.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in {outlanguage}. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level f{outlanguage} words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. The paragrah you will translate is {res_text}.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(translated[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chin-\\ntala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In\\nAdvances in Neural Information Processing Systems , volume 32. Curran Associates,\\nInc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/hash/\\nbdbca288fee7f92f2bfa9f7012727740-Abstract.html .\\n13', metadata={'source': 'test.pdf', 'page': 12}),\n",
       " Document(page_content='Appendix\\nA Reproducibility\\nA.1 Datasets\\nTable A.1: Details of eight real-world biomedical datasets used in this paper.\\nDataset name Samples Features Classes Samples per class\\nmeta-pam 1971 4160 2 1642, 329\\nlung 197 3312 4 139, 17, 21, 20\\ncll 111 11340 4 11, 49, 51\\nbreast 104 22293 2 62, 42\\nmpm 181 12533 2 150, 31\\ntoxicity 171 5748 4 45, 45, 39, 42\\nsmk 187 19993 2 90, 97\\nprostate 102 5966 2 50, 52\\nTable A.1 summarises all the datasets. Seven of the datasets are open-source and are available online:\\n‘breast’ [25],‘mpm’ [26] can be found at https://github.com/ramhiser/datamicroarray. : CLL-SUB-\\n111(called ‘cll’) [27],lung [28],Prostate GE (called ‘prostate’ ) [29],SMK-CAN-187 (called\\n‘smk’ ) [30] and TOX-171 (called ‘toxicity’ ) [31] can be found at (https://jundongl.github.io/scikit-\\nfeature/datasets.html). meta-pam is derived from the METABRIC dataset [ 32] by combining the\\nmolecular data with the clinical label ‘Pam50Subtype’. Because the label ‘Pam50Subtype’ was very\\nimbalanced, the task was transformed into a binary task of basal vs non-basal by combining the\\nclasses ‘LumA’, ‘LumB’, ‘Her2’, ‘Normal’ into one class and using the remaining class ‘Basal’ as\\nthe second class. The Hallmark gene set associated with breast cancer was selected and the final\\ndataset contained 1971 samples forming the “largest\" of the small-sample size datasets we looked at,\\nallowing us to more granularly investigate the effect of decreasing the number of training samples.\\nData preprocessing Before training the models, we normalise the dataset by scaling all features to\\nbe between 0 and 1 using scikit-learn.preprocessing.MinMaxScaler .\\nA.2 Computing Resources\\nWe trained over 20000 models on a Tesla P100-PCIE with 16GB memory with a Intel(R) Xeon(R)\\nGold 6142 CPU @ 2.60GHz 16-core processor. The operating system was Ubuntu 20.04.4 LTS. A\\nsmall portion of the training (<5%) was also performed using a RTX 3090 with 24GB memory with a\\nXeon ®E5-2609 v3 @ 1.90 GHz 16-core processor. We estimate that ∼400GPU-hours were used\\nto run all experiments.\\nA.3 Training details, Hyperparameters\\nSoftware Implementation We implemented all models using PyTorch[ 33], an open-source deep\\nlearning library with a BSD licence. All numerical plots and graphics have been generated using\\nMatplotlib, a Python-based plotting library with a BSD licence. The model architecture Figure 1 was\\ngenerated using draw.io, a free drawing software under Apache License 2.0. Our submitted code\\ncontains the exact version of all software and libraries we use.\\nWe attach our code to this submission, and we will release it under the MIT licence upon publication.\\nTraining details Below we present the most important training settings for each model.\\n•β-V AE and CAE each use encoder and decoder networks with two layers respectively of\\nsize 128,128.\\n•Forβ-V AE we use a monotonic βschedule, starting with β= 0at the start of training and\\nlinearly increasing up to its maximum value after 100 epochs.\\n14', metadata={'source': 'test.pdf', 'page': 13}),\n",
       " Document(page_content='Table C.10: Summary of performance of models on cll\\nFull data(n=66) Low data(n=33)\\nModel mean std mean std\\nβ-V AE 56.64 14.07 53.75 10.35\\nCAE 54.35 9.10 52.65 11.43\\nSubTab 54.32 11.16 58.04 12.88\\nEnV AE(k=2) 68.06 12.61 65.69 10.14\\nEnV AE(k=4) 67.22 11.55 67.64 10.49\\nEnV AE(k=6) 66.10 11.13 65.94 10.60\\nEnV AE(k=8) 66.35 11.73 65.25 10.30\\n18', metadata={'source': 'test.pdf', 'page': 17}),\n",
       " Document(page_content='mpm 95.21±4.38 95.21±4.38 95.21±4.38(k=6) 80.53±9.96 79 .33±11.69 89 .08±8.83\\ntoxicity 74.75±8.43 74.75±8.43 74.75±8.43(k=6) 50.83±9.31 52.35±10.85 74.16±8.23\\nsmk 67.15±8.14 67.15±8.14 67.15±8.14(k=8) 63.20±10.23 54.37±7.82 62.14±5.68\\nprostate 81.27±6.87 81.27±6.87 81.27±6.87(k=8) 73.16±9.86 56.49±8.27 66.34 ±14.47\\n5', metadata={'source': 'test.pdf', 'page': 4})]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
