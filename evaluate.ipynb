{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rMcGf3r6zby"
   },
   "source": [
    "# Question Answering over Resources\n",
    "The notebook is based on https://python.langchain.com/en/latest/modules/chains/index_examples/qa_with_sources.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAYYChTe6zb1"
   },
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FimqAIgc6zb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the token for Google Bard········\n",
      "Enter the token for OpenAI········\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "import getpass\n",
    "from bardapi import Bard\n",
    "import os\n",
    "\n",
    "bard_token = getpass.getpass(\"Enter the token for Google Bard\")\n",
    "openai_token = getpass.getpass(\"Enter the token for OpenAI\")\n",
    "# Input OpenAI API Key Here\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvJlaM8n7k_R",
    "outputId": "20f32fe4-2c9f-47d1-8567-0852f2e11e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('test_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "data = data['data']\n",
    "num = len(data)\n",
    "print(num)\n",
    "# print(data[0]['paragraphs'][0]['qas'][0])\n",
    "print(data[0]['paragraphs'][0]['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========\n",
    "# QUESTION: In what country is Normandy located?\n",
    "# ANSWER1: France\n",
    "# ANSWER2: Normandy is located in France.\n",
    "# RESPONSE: Yes\n",
    "# ==========\n",
    "# QUESTION: From which countries did the Norse originate?\n",
    "# ANSWER1: Denmark, Iceland and Norway \n",
    "# ANSWER2: The Norse who originated the Normans came from Denmark, Iceland, and Norway.\n",
    "# RESPONSE: Yes\n",
    "# ==========\n",
    "# QUESTION: What is France a region of?\n",
    "# ANSWER1: No relevant information found \n",
    "# ANSWER2: Sorry, I don't know.\n",
    "# RESPONSE: Yes\n",
    "# =========\n",
    "# QUESTION: What is France a region of?\n",
    "# ANSWER1: No relevant information found \n",
    "# ANSWER2: France is a region of Normandy.\n",
    "# RESPONSE: No\n",
    "# =========\n",
    "\n",
    "#Along with the answer, please also give a confidence score from 1 to 10 about how confident you are about your answer.\n",
    "\n",
    "eval_text_template = '''\n",
    "You are not allowed to use any outside knowledge. You can only use what is given to you. Given one question and two answers, please response whether these ANSWER1 and ANSWER2 have the same meaning under the given question. \n",
    "The only output you can give is either \"Yes\" or \"No\". Please output only one single word. No explanation needed.\n",
    "If ANSWER1 says \"No relevant information found\", you can only say \"yes\" when ANSWER2 also indicates the question cannot be answered. \n",
    "\n",
    "QUESTION: {question}\n",
    "ANSWER1: {answer}\n",
    "ANSWER2: {res_text}\n",
    "'''\n",
    "\n",
    "# ask_template = '''\n",
    "# Please answer the following question only based on the given context. Do NOT use any outside knowledge. \n",
    "# If you are unsure about the answer or you cannot find the information to answer your question, you must say \"Sorry, I cannot answer this question\"\n",
    "# ==========\n",
    "# QUESTION: In what country is Normandy located?\n",
    "# RESPONSE: France\n",
    "# ==========\n",
    "\n",
    "# QUESTION: What is France a region of?\n",
    "# RESPONSE: Sorry, I cannot answer this question\n",
    "# =========\n",
    "# QUESTION: {question}\n",
    "# RESPONSE: \n",
    "# '''\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based on the context below. You are NOT allowed to use any outside information. If the question cannot be answered using the information provided, you must answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"query\"],\n",
    "    template=template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_NhDW6QV6zb3"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 48\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;66;03m# long text----- search similarity first\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#             embeddings = OpenAIEmbeddings()\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#             text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=0)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#             in_text = prompt_template.format(context=text, query=question)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#             res_text = model(in_text)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m             prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI want you to work as a question answering machine over given documents. The documents you will be given is the following paragraph: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Then, I want to you answer the question based on the given paragraph. The question is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 48\u001b[0m             res_text2 \u001b[38;5;241m=\u001b[39m \u001b[43mbard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#             print(res_text+\"||||\"+res_text2)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#             time.sleep(1)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#             is_same = evaluator.get_answer(eval_text_template.format(question=question, answer=res_text2, res_text=res_text))['content']\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#                 else:\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#                     hallucination.append(0)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124manswer:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mresponse:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres_text2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa/lib/python3.11/site-packages/bardapi/core.py:119\u001b[0m, in \u001b[0;36mBard.get_answer\u001b[0;34m(self, input_text)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    117\u001b[0m parsed_answer \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(resp_dict)\n\u001b[1;32m    118\u001b[0m bard_answer \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mparsed_answer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversation_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: parsed_answer[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: parsed_answer[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfactualityQueries\u001b[39m\u001b[38;5;124m\"\u001b[39m: parsed_answer[\u001b[38;5;241m3\u001b[39m],\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtextQuery\u001b[39m\u001b[38;5;124m\"\u001b[39m: parsed_answer[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m parsed_answer[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: i[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: i[\u001b[38;5;241m1\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m parsed_answer[\u001b[38;5;241m4\u001b[39m]],\n\u001b[1;32m    125\u001b[0m }\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversation_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoice_id \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    127\u001b[0m     bard_answer[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversation_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    128\u001b[0m     bard_answer[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    129\u001b[0m     bard_answer[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    130\u001b[0m )\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reqid \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from bardapi import Bard\n",
    "import time\n",
    "hallucination = []\n",
    "acc = []\n",
    "\n",
    "# model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')\n",
    "model = OpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\",)\n",
    "chain = load_qa_with_sources_chain(model, chain_type=\"stuff\")\n",
    "# chain = load_qa_with_sources_chain(model, chain_type=\"map_reduce\")\n",
    "evaluator = Bard(token=bard_token)\n",
    "bard = Bard(token=bard_token)\n",
    "for i in range(num):\n",
    "    for j in range(len(data[0]['paragraphs'])):\n",
    "        text= data[i]['paragraphs'][j]['context']\n",
    "#         text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "#         texts = text_splitter.split_text(text)\n",
    "#         embeddings = OpenAIEmbeddings()\n",
    "#         docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))])\n",
    "        for k in range(len(data[i]['paragraphs'][j]['qas'])):\n",
    "            question = data[i]['paragraphs'][j]['qas'][k]['question']\n",
    "            if not data[i]['paragraphs'][j]['qas'][k]['is_impossible']:\n",
    "                answer = data[i]['paragraphs'][j]['qas'][k]['answers'][0]['text']\n",
    "                pos = data[i]['paragraphs'][j]['qas'][k]['answers'][0]['answer_start']\n",
    "            else:\n",
    "                answer = \"I don't know\"\n",
    "                pos = None\n",
    "            query = question\n",
    "            \n",
    "            # long text----- search similarity first\n",
    "#             embeddings = OpenAIEmbeddings()\n",
    "#             text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=0)\n",
    "#             pages = text_splitter.split_text(text)\n",
    "#             db = Chroma.from_texts(pages, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(pages))])\n",
    "#             docs = db.similarity_search(query)\n",
    "#             model = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "#             chain = load_qa_with_sources_chain(model, chain_type=\"map_reduce\")\n",
    "#             res_text = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "            \n",
    "#             short text---- put everything in\n",
    "#             in_text = prompt_template.format(context=text, query=question)\n",
    "#             res_text = model(in_text)\n",
    "            \n",
    "            \n",
    "            \n",
    "            prompt = f\"I want you to work as a question answering machine over given documents. The documents you will be given is the following paragraph: {text}. Then, I want to you answer the question based on the given paragraph. The question is {query}\"\n",
    "            res_text2 = bard.get_answer(prompt)['content']\n",
    "#             print(res_text+\"||||\"+res_text2)\n",
    "#             time.sleep(1)\n",
    "#             is_same = evaluator.get_answer(eval_text_template.format(question=question, answer=res_text2, res_text=res_text))['content']\n",
    "#             if 'yes' not in is_same.lower():\n",
    "#                 res_text = \"I don't know\"\n",
    "#             time.sleep(0.5)\n",
    "#             result = evaluator.get_answer(eval_text_template.format(question=question, answer=answer, res_text=res_text))['content']\n",
    "#             if not data[i]['paragraphs'][j]['qas'][k]['is_impossible']:\n",
    "#                 if 'yes' in result.lower():\n",
    "#                     acc.append(1)\n",
    "#                 else:\n",
    "#                     acc.append(0)\n",
    "#             else:\n",
    "#                 if 'yes' in result.lower():\n",
    "#                     hallucination.append(1)\n",
    "#                 else:\n",
    "#                     hallucination.append(0)\n",
    "            print(f\"question: {question}\\nanswer:{answer} \\nresponse:{res_text2}\\n \\n\\n\\n\")\n",
    "#         print(f\"acc: {np.mean(acc)}, hallucination: {np.mean(hallucination)}\\n\\n\\n\")\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTJ4WEkX6zb4"
   },
   "source": [
    "## Bard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bardapi import Bard\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When were the Normans in Normandy?\"\n",
    "response = \"No relevant information found\"\n",
    "solution = \"The Normans gave their name to Normandy in the 10th and 11th centuries.\"\n",
    "\n",
    "\n",
    "\n",
    "Bard(token = token).get_answer(f\"Given the question:'{question}', do the following 2 statements have the same meaning? 1.{solution}, 2.{response}. Say yes if true. Otherwise, say no.\")['content']\n",
    "\n",
    "\n",
    "\n",
    "question = \"When was the French version of the word Norman first recorded?\"\n",
    "ans1 = \"No relevant information found \"\n",
    "ans2 = \"The French version of the word Norman was first recorded in Medieval Latin in the 9th century.\"\n",
    "\n",
    "res = bard.get_answer(f\"Answer the following question in a **single word**. Remember, **a single word**. Based on {question}, does {ans1} mean the same thing as {ans2}?\")['content']\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json, collections, time, re, string, os\n",
    "from datetime import datetime\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_url(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup\n",
    "def extract_nature_articles():\n",
    "    \"\"\"\n",
    "    Search for and parse all coronavirus-related News article from the Nature journal that were\n",
    "    published in a given period\n",
    "    \n",
    "    args:\n",
    "        start_date (str): the lower bound of the date range to filter articles,\n",
    "            has the format yyyy-mm-dd\n",
    "        end_date (str): the upper bound (inclusive) of the date range to filter articles,\n",
    "            has the format yyyy-mm-dd\n",
    "    \n",
    "    kwargs:\n",
    "        base_url (str): the home page url of Nature\n",
    "    \n",
    "    return:\n",
    "        List[str] : a list of article titles that meet the search criteria, ordered by\n",
    "            date and by title\n",
    "    \"\"\"\n",
    "    result = collections.OrderedDict()\n",
    "    url = \"https://www.nature.com/search?q=coronavirus&journal=nature&article_type=news&date_range=2023-2023&order=date_asc&title=coronavirus\"\n",
    "    soup = retrieve_url(url)\n",
    "    article_list = soup.find('ul', {'class':'app-article-list-row'}).find_all('li', {'class': \"app-article-list-row__item\"})\n",
    "    for item in article_list:\n",
    "        time = item.find('div', {'class': \"c-card__section\"}).find('time', {'class': 'c-meta__item'})['datetime']\n",
    "    \n",
    "        title = item.find('h3', {'class': \"c-card__title\"}).text.strip()\n",
    "        if time in result:\n",
    "            result[time].append(title)\n",
    "        else:\n",
    "            result[time] = [title]\n",
    "    temp = []\n",
    "    for titles in result.values():\n",
    "        temp.extend(sorted(titles))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coronavirus variant XBB.1.5 rises in the United States — is it a global threat?',\n",
       " 'The next worrisome coronavirus variant could come from China — will it get detected?',\n",
       " 'The next generation of coronavirus vaccines: a graphical guide',\n",
       " 'NIH reinstates grant for controversial coronavirus research']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_nature_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "New SARS-CoV-2 variants arise and spread with great stealth, but that hasn’t stopped Africa’s genomic sleuths from spotting a host of these threats — and alerting the rest of the world. Now an analysis details how the rapid growth in Africa’s sequencing capacity has aided global SARS-CoV-2 surveillance1. It also reveals that most variants were imported into Africa more often than they were exported from the continent.\n",
    "\n",
    "The study, published in Science, shows that “African scientists can work together to produce high-level science”, says co-author Tulio de Oliveira, a bioinformatician at Stellenbosch University in South Africa. “Before, it was almost the norm that African scientists would work with a northern partner to produce that kind of level of science.”\n",
    "\n",
    "Homegrown talent\n",
    "In March 2020, during the early days of the pandemic, fewer than 15 of the 55 countries recognized by the African Union had the necessary infrastructure to sequence genomes. Now 39 have their own sequencing facilities. And what began as a small group of researchers in Africa meeting virtually has ballooned into a consortium involving more than 400 scientists and public-health officials from across the continent and beyond working together to track the spread of SARS-CoV-2 variants and monitor changes in the variants’ genomes.\n",
    "\n",
    "As the consortium grew, so too did the number of sequenced genomes from Africa. By mid-2021, group members had sequenced more than 10,000 SARS-CoV-2 genomes collected on the continent2. By March this year, the count had reached 100,000.\n",
    "\n",
    "The authors note that this marks a major milestone in African genomic surveillance. By comparison, only around 3,700 whole HIV genomes from Africa have been sequenced and publicly shared, although the virus has been circulating for decades.\n",
    "\n",
    "“This paper is incredible,” says Jeremy Kamil, a virologist at Louisiana State University Health Shreveport. “The world needs to see more collaborations like this.”\n",
    "\n",
    "Genomes from a total of 52 African countries have been deposited in GISAID, an online repository that is the world’s largest digital database of SARS-CoV-2 sequences. Some genomes were sequenced in the countries where they were sampled, some in other African countries and others in laboratories outside Africa.\n",
    "\n",
    "The researchers found that in-country sequencing offered a clear speed advantage. The median turnaround — the time between the collection of a specimen from an infected individual and the addition of a completed sequence to GISAID — for genomes sequenced locally was 51 days. Median turnaround was 90 days for samples sent to other African countries and 113 days for those sent to facilities elsewhere in the world (see ‘Home-ground advantage’).\n",
    "\n",
    "These findings “jumped out at me right away”, says Kamil. Speed is crucial when it comes to fighting the virus, he says. If a variant such as Omicron is allowed to spread for even two or three extra weeks before it’s noticed, the world’s already-slow systems for updating vaccines will have even less time to catch up, Kamil adds.\n",
    "\n",
    "The collection of 100,000 SARS-CoV-2 genomes enabled de Oliveira and his colleagues to map when and where variants were introduced into Africa and how they had spread. The Omicron variant, for example, was first detected in South Africa in November 2021. The researchers found that although the Omicron subvariant BA.1 was exported from Africa at least 55 times — primarily to Europe and North America — it was imported into Africa at least 69 times from Europe and 102 times from North America (see ‘Virus on the go’). These import events brought the variant to African countries outside of the southern part of the continent, says study co-author Eduan Wilkinson, a bioinformatician at Stellenbosch University.\n",
    "\n",
    "Overall, the team’s analyses indicated that most SARS-CoV-2 variants were more often introduced into Africa from other parts of the world than the other way around. “The ironic part was that Africa was punished a few times from discovery of variants,” de Oliveira says. “But a great majority of the variants, including most of the introductions of Omicron, did not come from Africa.”\n",
    "\n",
    "De Oliveira and his colleagues plan to adapt the existing sequencing infrastructure to monitor other infectious viruses and bacteria that are of concern in Africa, such as the tuberculosis bacterium, HIV and the Lassa and Ebola viruses. “We have a lot of pathogens to deal with,” de Oliveira says.\n",
    "\"\"\"\n",
    "\n",
    "text2 = \"\"\"\n",
    "The US National Institutes of Health (NIH) has reinstated a grant to a highly scrutinized research organization that studies bat coronaviruses — but the agency has placed several stipulations on the scope of the research and on the organization’s accounting practices.\n",
    "\n",
    "The move caps a years-long saga that has thrust the EcoHealth Alliance, a small non-profit organization in New York City, into the political fray for its collaborations with the Wuhan Institute of Virology (WIV) in China. In April 2020, after then-US-president Donald Trump hinted that SARS-CoV-2 originated in a WIV laboratory, the NIH terminated EcoHealth’s grant. Its goal was to study how coronaviruses, such as SARS-CoV-2, jump from bats to humans. A few months later, the NIH reinstated and immediately suspended the award until certain conditions were met that, at the time, EcoHealth said were impossible to complete.\n",
    "\n",
    "Researchers who spoke to Nature applaud the renewal, adding that this type of research is essential to avert the next pandemic. They claim that the NIH’s termination and subsequent suspension were politically motivated, and that, although long overdue, this renewal ends — for now — a drama-filled exchange between the agency and EcoHealth.\n",
    "\n",
    "“It’s about goddam time,” says Gerald Keusch, associate director of the National Emerging Infectious Diseases Laboratory at Boston University in Massachusetts, who organized researchers to push back against EcoHealth’s grant termination in 2020. “The integrity of science requires a barrier against political interference,” he says.\n",
    "\n",
    "The NIH “routinely considers processes and measures for strengthening [its] oversight over federal funds” and has been working with EcoHealth to strengthen its “administrative processes to meet NIH’s expectations”, says Amanda Fine, a spokesperson for the NIH in Washington DC.\n",
    "\n",
    "Lengthy list of conditions\n",
    "Although the organization will now be able to continue its bat coronavirus research for the first time since the saga began, the NIH placed an extensive list of restrictions on the four-year, US$2.9-million award. None of the researchers who spoke to Nature had ever seen a grant with so many stipulations.\n",
    "\n",
    "Among other things, EcoHealth is specifically forbidden from performing any in-country research in China, including with the WIV, or collecting any new samples from vertebrates — such as bats. The revised grant also mandates greater scrutiny of EcoHealth’s finances and accounting practices, driven in part by a federal watchdog report, released in January, finding that EcoHealth had misreported about $90,000 in expenses. The report also faulted the NIH for improperly terminating EcoHealth’s grant.\n",
    "\n",
    "Furthermore, EcoHealth will be forbidden from performing any work that is deemed by the NIH’s parent organization, the Department of Health and Human Services (HHS), to have the potential to enhance the virulence or transmission of a virus. This restriction stems, in part, from criticism that research done at the WIV and funded by an EcoHealth subaward qualified as ‘gain of function’ research.\n",
    "\n",
    "Disputed research\n",
    "Congressional Republicans have alleged that this research, which involved attaching spike proteins from wild bat coronaviruses to an unrelated virus to determine whether the wild pathogens could infect human airway cells, should have undergone HHS review. Anthony Fauci, then-director of the National Institute for Allergy and Infectious Diseases, has said the agency concluded that these experiments did not meet the bar to undergo such review, and noted that the WIV did not intend to enhance the viruses.\n",
    "\n",
    "Virologists say this type of research is essential for developing vaccines and therapeutics against emerging pathogens and for understanding how likely a pathogen is to spark a pandemic. The NIH and HHS have been finalizing guidance that will probably tighten the oversight of such research in the United States.\n",
    "\n",
    "“I don’t know if any other single grantee from NIH has been subjected to this level of oversight,” says Peter Daszak, president of the EcoHealth Alliance. Yet he is “positive and optimistic” about the grant restart, despite all the restrictions. A key priority for the newly released funds will be analysing nearly 300 partial or complete genomes of SARS-related coronaviruses from samples that the organization collected before the funding halt, he says.\n",
    "\n",
    "Third rail of virology\n",
    "These restrictions seem reasonable, in light of the enormous public attention to and scrutiny of gain-of-function research, says Lawrence Gostin, a health-law and policy specialist at Georgetown University in Washington DC. Still, Gostin says he is surprised the agency restarted its funding for EcoHealth, given that it has been the “third rail of politics” the past few years.\n",
    "\n",
    "Angela Rasmussen, a virologist at the University of Saskatchewan in Saskatoon, Canada, says she is pleasantly surprised to see the grant renewed, but worries about the “terrible precedent” that the NIH set by “arbitrarily” terminating an award on the basis of “unfounded rumours” regarding the origins of SARS-CoV-2. She hopes that these same restrictions will not apply to other scientists doing similar work, but is encouraged by the number of research groups that are now studying coronaviruses following the COVID-19 pandemic.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from bardapi import Bard\n",
    "import time\n",
    "model = OpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\",)\n",
    "chain = load_qa_with_sources_chain(model, chain_type=\"stuff\")\n",
    "\n",
    "question = \"How many countries recognized by the African Union had the necessary infrastructure to sequence genomes.\"\n",
    "question = 'Summarize every single given article separately.'\n",
    "inlanguage = \"Chinese\"\n",
    "translated = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"You are a {inlanguage} translator.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in {inlanguage}. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level f{inlanguage} words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. The paragrah you will translate is {question}.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question = translated[\"choices\"][0][\"message\"][\"content\"]\n",
    "in_text = prompt_template.format(context=text1+\"\\n=====\\n\"+text2, query=question)\n",
    "res_text = model(in_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一篇文章讨论了非洲的基因组测序能力的增长，使非洲科学家能够追踪和监测SARS-CoV-2变异体的传播。它还强调，大多数变异体是从其他地区输入非洲而非出口到这个大陆的。第二篇文章讨论了对EcoHealth联盟这个研究机构恢复了一项资助，该机构专门研究蝙蝠冠状病毒。该资助附带了几个限制条件，包括对中国国内研究的限制以及对该组织财务情况的更严格审查。\n"
     ]
    }
   ],
   "source": [
    "outlanguage = \"Chinese\"\n",
    "translated = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"You are a {outlanguage} translator.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in {outlanguage}. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level f{outlanguage} words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. The paragrah you will translate is {res_text}.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(translated[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The first article discusses how Africa's genomic sequencing capacity has grown, allowing African scientists to track and monitor the spread of SARS-CoV-2 variants. It also highlights that most variants were imported into Africa rather than exported from the continent. The second article discusses the reinstatement of a grant to the EcoHealth Alliance, a research organization studying bat coronaviruses. The grant comes with several stipulations, including restrictions on in-country research in China and greater scrutiny of the organization's finances.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Enhancing Representation Learning on\\nHigh-Dimensional, Small-Size Tabular Data:\\nA Divide and Conquer Method with Ensembled V AEs\\nNavindu Leelarathna\\nUniversity of Cambridge\\nnyl25@cam.ac.ukAndrei Margeloiu\\nUniversity of Cambridge\\nam2770@cam.ac.ukMateja Jamnik\\nUniversity of Cambridge\\nmj201@cam.ac.uk\\nNikola Simidjievski\\nUniversity of Cambridge\\nns779@cam.ac.uk\\nAbstract\\nVariational Autoencoders and their many variants have displayed impressive ability\\nto perform dimensionality reduction, often achieving state-of-the-art performance.\\nMany current methods however, struggle to learn good representations in High\\nDimensional, Low Sample Size (HDLSS) tasks, which is an inherently challenging\\nsetting. We address this challenge by using an ensemble of lightweight V AEs\\nto learn posteriors over subsets of the feature-space, which get aggregated into a\\njoint posterior in a novel divide-and-conquer approach. Specifically, we present an\\nalternative factorisation of the joint posterior that induces a form of implicit data\\naugmentation that yields greater sample efficiency. Through a series of experiments\\non eight real-world datasets, we show that our method learns better latent repre-\\nsentations in HDLSS settings, which leads to higher accuracy in a downstream\\nclassification task. Furthermore, we verify that our approach has a positive effect\\non disentanglement and achieves a lower estimated Total Correlation on learnt\\nrepresentations. Finally, we show that our approach is robust to partial features at in-\\nference, exhibiting little performance degradation even with most features missing.\\n1 Introduction\\nHigh Dimensional, Low Sample Size (HDLSS) data occurs ubiquitously in clinical research and gene\\nexpression analysis but is challenging to work with for two reasons. Firstly, as a consequence of high-\\ndimensionality, raw data often only sparsely covers the input space, an issue commonly referred to as\\nthecurse of dimensionality . Secondly, analysing such data is often computationally challenging due\\nto the size of models required and vast quantities of training data needed. Dimensionality reduction\\nhas emerged as a hugely popular technique to address this issue and is often used as a preprocessing\\nstep to transform high-dimensional data into a lower-dimensional space (whilst retaining meaningful\\ninformation) that is more readily suitable for downstream tasks.\\nIn particular, V AEs [ 1] and their variants have emerged as a dominant architecture for dimensionality\\nreduction and have displayed an impressive performance in recent years. However, the curse of\\ndimensionality remains, and the majority of current approaches still require large volumes of input\\ndata to learn high-quality latent representations. As a result, many state-of-the-art models struggle\\nwith HDLSS data and either overfit or fail to learn high-quality representations, which we attribute to\\noverparameterisation and low sample-efficiency.\\nPreprint. Under review.arXiv:2306.15661v1  [cs.LG]  27 Jun 2023', metadata={'source': 'test.pdf', 'page': 0})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"test.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper introduces a method called EnV AE, which is an ensemble of lightweight Variational Autoencoders (VAEs) trained on subsets of the feature-space. This approach improves representation learning in high-dimensional, low sample size (HDLSS) tasks and leads to higher accuracy in downstream classification tasks. EnV AE also demonstrates better disentanglement and robustness to missing features. The method outperforms other benchmark models and introduces minimal computational overhead. The paper provides details on the implementation and performance of EnV AE on various datasets.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "chain = load_summarize_chain(ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0), chain_type=\"map_reduce\")\n",
    "chain.run(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_text': 'The content of the article includes information about PyTorch, a deep learning library, and its use in training models on various biomedical datasets. The article also provides details about the datasets used, computing resources, training details, and performance of the models on specific datasets. The article emphasizes the use of PyTorch and the training settings for each model. Additionally, the article mentions the normalization of the dataset using scikit-learn.preprocessing.MinMaxScaler and the use of specific hardware for training the models. The article concludes with a summary of the performance of the models on different datasets.\\nSOURCES: test.pdf'}\n"
     ]
    }
   ],
   "source": [
    "question = \"总结这篇文章的内容\"\n",
    "\n",
    "inlanguage = \"Chinese\"\n",
    "translated = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"You are a {inlanguage} translator.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in {inlanguage}. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level f{language} words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. The paragrah you will translate is {question}.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question = translated[\"choices\"][0][\"message\"][\"content\"]\n",
    "in_text = prompt_template.format(context=text, query=question)\n",
    "res_text = model(in_text)\n",
    "\n",
    "outlanguage = \"Chinese\"\n",
    "translated = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"You are a {outlanguage} translator.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in {outlanguage}. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level f{outlanguage} words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. The paragrah you will translate is {res_text}.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(translated[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chin-\\ntala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In\\nAdvances in Neural Information Processing Systems , volume 32. Curran Associates,\\nInc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/hash/\\nbdbca288fee7f92f2bfa9f7012727740-Abstract.html .\\n13', metadata={'source': 'test.pdf', 'page': 12}),\n",
       " Document(page_content='Appendix\\nA Reproducibility\\nA.1 Datasets\\nTable A.1: Details of eight real-world biomedical datasets used in this paper.\\nDataset name Samples Features Classes Samples per class\\nmeta-pam 1971 4160 2 1642, 329\\nlung 197 3312 4 139, 17, 21, 20\\ncll 111 11340 4 11, 49, 51\\nbreast 104 22293 2 62, 42\\nmpm 181 12533 2 150, 31\\ntoxicity 171 5748 4 45, 45, 39, 42\\nsmk 187 19993 2 90, 97\\nprostate 102 5966 2 50, 52\\nTable A.1 summarises all the datasets. Seven of the datasets are open-source and are available online:\\n‘breast’ [25],‘mpm’ [26] can be found at https://github.com/ramhiser/datamicroarray. : CLL-SUB-\\n111(called ‘cll’) [27],lung [28],Prostate GE (called ‘prostate’ ) [29],SMK-CAN-187 (called\\n‘smk’ ) [30] and TOX-171 (called ‘toxicity’ ) [31] can be found at (https://jundongl.github.io/scikit-\\nfeature/datasets.html). meta-pam is derived from the METABRIC dataset [ 32] by combining the\\nmolecular data with the clinical label ‘Pam50Subtype’. Because the label ‘Pam50Subtype’ was very\\nimbalanced, the task was transformed into a binary task of basal vs non-basal by combining the\\nclasses ‘LumA’, ‘LumB’, ‘Her2’, ‘Normal’ into one class and using the remaining class ‘Basal’ as\\nthe second class. The Hallmark gene set associated with breast cancer was selected and the final\\ndataset contained 1971 samples forming the “largest\" of the small-sample size datasets we looked at,\\nallowing us to more granularly investigate the effect of decreasing the number of training samples.\\nData preprocessing Before training the models, we normalise the dataset by scaling all features to\\nbe between 0 and 1 using scikit-learn.preprocessing.MinMaxScaler .\\nA.2 Computing Resources\\nWe trained over 20000 models on a Tesla P100-PCIE with 16GB memory with a Intel(R) Xeon(R)\\nGold 6142 CPU @ 2.60GHz 16-core processor. The operating system was Ubuntu 20.04.4 LTS. A\\nsmall portion of the training (<5%) was also performed using a RTX 3090 with 24GB memory with a\\nXeon ®E5-2609 v3 @ 1.90 GHz 16-core processor. We estimate that ∼400GPU-hours were used\\nto run all experiments.\\nA.3 Training details, Hyperparameters\\nSoftware Implementation We implemented all models using PyTorch[ 33], an open-source deep\\nlearning library with a BSD licence. All numerical plots and graphics have been generated using\\nMatplotlib, a Python-based plotting library with a BSD licence. The model architecture Figure 1 was\\ngenerated using draw.io, a free drawing software under Apache License 2.0. Our submitted code\\ncontains the exact version of all software and libraries we use.\\nWe attach our code to this submission, and we will release it under the MIT licence upon publication.\\nTraining details Below we present the most important training settings for each model.\\n•β-V AE and CAE each use encoder and decoder networks with two layers respectively of\\nsize 128,128.\\n•Forβ-V AE we use a monotonic βschedule, starting with β= 0at the start of training and\\nlinearly increasing up to its maximum value after 100 epochs.\\n14', metadata={'source': 'test.pdf', 'page': 13}),\n",
       " Document(page_content='Table C.10: Summary of performance of models on cll\\nFull data(n=66) Low data(n=33)\\nModel mean std mean std\\nβ-V AE 56.64 14.07 53.75 10.35\\nCAE 54.35 9.10 52.65 11.43\\nSubTab 54.32 11.16 58.04 12.88\\nEnV AE(k=2) 68.06 12.61 65.69 10.14\\nEnV AE(k=4) 67.22 11.55 67.64 10.49\\nEnV AE(k=6) 66.10 11.13 65.94 10.60\\nEnV AE(k=8) 66.35 11.73 65.25 10.30\\n18', metadata={'source': 'test.pdf', 'page': 17}),\n",
       " Document(page_content='mpm 95.21±4.38 95.21±4.38 95.21±4.38(k=6) 80.53±9.96 79 .33±11.69 89 .08±8.83\\ntoxicity 74.75±8.43 74.75±8.43 74.75±8.43(k=6) 50.83±9.31 52.35±10.85 74.16±8.23\\nsmk 67.15±8.14 67.15±8.14 67.15±8.14(k=8) 63.20±10.23 54.37±7.82 62.14±5.68\\nprostate 81.27±6.87 81.27±6.87 81.27±6.87(k=8) 73.16±9.86 56.49±8.27 66.34 ±14.47\\n5', metadata={'source': 'test.pdf', 'page': 4})]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
