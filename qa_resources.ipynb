{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering over Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "import getpass \n",
    "from bardapi import Bard\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize OpenAI & Bard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "bard_token = getpass.getpass(\"Enter the token for Google Bard: \")\n",
    "bard = Bard(token=bard_token)\n",
    "\n",
    "openai_key = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "os.environ['OPENAI_API_KEY'] = openai_key\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TXT Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Test-Documents/state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "db = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"Test-Documents/state_of_the_union.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "db = Chroma.from_documents(pages, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did president say about China?\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "para = \"\"\n",
    "for each in docs: \n",
    "    para += each.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangpan/Library/Python/3.9/lib/python/site-packages/langchain/llms/openai.py:171: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/yangpan/Library/Python/3.9/lib/python/site-packages/langchain/llms/openai.py:716: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = OpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "chain = load_qa_with_sources_chain(model, chain_type=\"stuff\")\n",
    "openai_res = chain({\"input_documents\": pages, \"question\": query}, return_only_outputs=True)\n",
    "\n",
    "prompt = f\"I want you to work as a question answering machine over given documents. The documents you will be given is the following paragraph: {para}. Then, I want to you answer the question based on the given paragraph. The question is {query}\"\n",
    "bard_res = bard.get_answer(prompt)['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The president did not mention China.\n",
      "SOURCES: Test-Documents/state_of_the_union.pdf\n",
      "\n",
      "The president said that China is a competitor and that we need to level the playing field with them. He also said that we need to pass the Bipartisan Innovation Act to make record investments in emerging technologies and American manufacturing.\n",
      "\n",
      "Here are the specific quotes from the paragraph:\n",
      "\n",
      "* \"But to compete for the best jobs of the future, we also need to level the playing field with China and other competitors.\"\n",
      "* \"That’s why it is so important to pass the Bipartisan Innovation Act sitting in Congress that will make record investments in emerging technologies and American manufacturing.\"\n",
      "\n",
      "The president did not say anything negative about China in the paragraph. He simply stated that they are a competitor and that we need to take steps to ensure that we are competitive with them.\n"
     ]
    }
   ],
   "source": [
    "print(openai_res['output_text'] + \"\\n\")\n",
    "print(bard_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Translation Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El presidente afirmó que China es un competidor y necesitamos nivelar el campo de juego con ellos y otros competidores para poder competir por los mejores trabajos del futuro. Es por ello que es de suma importancia que se apruebe el Acta de Innovación Bipartidista en el Congreso, la cual propone realizar inversiones sin precedentes en tecnologías emergentes y la manufacturación estadounidense. El presidente no emitió comentarios negativos respecto a China en el párrafo. Simplemente, expresó la necesidad de tomar medidas para asegurar que seamos competitivos con ellos.\n"
     ]
    }
   ],
   "source": [
    "language = \"Spanish\"\n",
    "translated = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"You are a {language} translator.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in {language}. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level f{language} words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. The paragrah you will translate is {bard_res}.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(translated[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highlight Original Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
